{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attribute_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1S385uE-qL65CyF_dl-fYFNYm7zKeRdxB",
      "authorship_tag": "ABX9TyNQ3jyChietzmJP/jOowmr/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v83GZl6ndk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7b1358e5-d64f-42d5-eb69-c1e9c8bf2355"
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoQmvmvJTbvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch as nn\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH-ygOMWKGLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "201a84ca-e542-4459-df1c-b8dc4fb58816"
      },
      "source": [
        "#google drive, google cloud details\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/gdrive/My Drive/Colab Notebooks/cred_details/gmds-2608-f8272c44cb11.json\"\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]=\"/content/gdrive/My Drive/Colab Notebooks/cred_details/gmds-2608-f8272c44cb11.json\"\n",
        "project_id='gmds-2608'\n",
        "auth.authenticate_user()\n",
        "gcs_service = build('storage', 'v1')\n",
        "bucket_name = 'attribute_assignment'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 42, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQSGlhvmuJ8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b3e425ae-b2a2-41a0-f823-bb0105bb6e3a"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productMeta.txt.gz\n",
        "!wget http://snap.stanford.edu/data/amazon/categories.txt.gz "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-08 04:10:59--  http://snap.stanford.edu/data/amazon/productMeta.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1897640515 (1.8G) [application/x-gzip]\n",
            "Saving to: ‘productMeta.txt.gz’\n",
            "\n",
            "productMeta.txt.gz   18%[==>                 ] 331.34M  4.98MB/s    eta 5m 21s ^C\n",
            "--2020-07-08 04:12:11--  http://snap.stanford.edu/data/amazon/categories.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46259345 (44M) [application/x-gzip]\n",
            "Saving to: ‘categories.txt.gz’\n",
            "\n",
            "categories.txt.gz   100%[===================>]  44.12M  13.5MB/s    in 3.3s    \n",
            "\n",
            "2020-07-08 04:12:15 (13.5 MB/s) - ‘categories.txt.gz’ saved [46259345/46259345]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYWClsgot_dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "media = MediaFileUpload('productMeta.txt.gz', mimetype='application/gzip', resumable=True)\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, name='productMeta.txt.gz', media_body=media)\n",
        "response = None\n",
        "while response is None:\n",
        "    _, response = request.next_chunk()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogs4Y1AAnsgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "media = MediaFileUpload('categories.txt.gz', mimetype='application/gzip', resumable=True)\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, name='categories.txt.gz', media_body=media)\n",
        "response = None\n",
        "while response is None:\n",
        "    _, response = request.next_chunk()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpnu6WF64XBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "09f5b46f-dc04-4a51-9e49-ef20165c069a"
      },
      "source": [
        "!pip install keras-self-attention\n",
        "!pip install keras-word-char-embd\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "#!cp '/content/drive/My Drive/Colab Notebooks/attribute_assignment/model.py' ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.46.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: keras-word-char-embd in /usr/local/lib/python3.6/dist-packages (0.20.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-word-char-embd) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-word-char-embd) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (1.12.0)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-dc_ua1mh\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-dc_ua1mh\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=bdefbf6582afcff5b0732b3113baf3d54e703892207d3ff8396d8bbe18737d2e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0emvybqg/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJA5J32spBNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_wc_embd import get_dicts_generator, get_batch_input\n",
        "from model import build_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpP-Y7volBmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from model import build_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYXdSKUH4E0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = '/content/model.h5'\n",
        "DATA_ROOT = 'CoNNL2003eng'\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_ROOT, 'train.txt')\n",
        "DATA_VALID_PATH = os.path.join(DATA_ROOT, 'valid.txt')\n",
        "DATA_TEST_PATH = os.path.join(DATA_ROOT, 'test.txt')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj1ZcswYRWJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORD_EMBD_PATH = 'glove.6B.100d.txt'\n",
        "TAGS = {\n",
        "    'O': 0,\n",
        "    'B-PER': 1,\n",
        "    'I-PER': 2,\n",
        "    'B-LOC': 3,\n",
        "    'I-LOC': 4,\n",
        "    'B-ORG': 5,\n",
        "    'I-ORG': 6,\n",
        "    'B-MISC': 7,\n",
        "    'I-MISC': 8,\n",
        "}\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoSnGpJCd6TC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f51f955a-9387-4297-dd25-98dd167dad0c"
      },
      "source": [
        "def load_data(path):\n",
        "    sentences, taggings = [], []\n",
        "    with codecs.open(path, 'r', 'utf8') as reader:\n",
        "        for line in reader:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if not sentences or len(sentences[-1]) > 0:\n",
        "                    sentences.append([])\n",
        "                    taggings.append([])\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if parts[0] != '-DOCSTART-':\n",
        "                sentences[-1].append(parts[0])\n",
        "                taggings[-1].append(TAGS[parts[-1]])\n",
        "    if not sentences[-1]:\n",
        "        sentences.pop()\n",
        "        taggings.pop()\n",
        "    return sentences, taggings\n",
        "\n",
        "\n",
        "print('Loading...')\n",
        "train_sentences, train_taggings = load_data(DATA_TRAIN_PATH)\n",
        "valid_sentences, valid_taggings = load_data(DATA_VALID_PATH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwz_1vIKeCz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22a65dc8-7560-480e-8af2-cb45b2367038"
      },
      "source": [
        "dicts_generator = get_dicts_generator(\n",
        "    word_min_freq=2,\n",
        "    char_min_freq=1e100,\n",
        "    word_ignore_case=True,\n",
        "    char_ignore_case=False\n",
        ")\n",
        "for sentence in train_sentences:\n",
        "    dicts_generator(sentence)\n",
        "word_dict, _, _ = dicts_generator(return_dict=True)\n",
        "\n",
        "if os.path.exists(WORD_EMBD_PATH):\n",
        "    print('Embedding...')\n",
        "    word_dict = {\n",
        "        '': 0,\n",
        "        '<UNK>': 1,\n",
        "    }\n",
        "    word_embd_weights = [\n",
        "        [0.0] * 100,\n",
        "        numpy.random.random((100,)).tolist(),\n",
        "    ]\n",
        "    with codecs.open(WORD_EMBD_PATH, 'r', 'utf8') as reader:\n",
        "        for line_num, line in enumerate(reader):\n",
        "            if (line_num + 1) % 1000 == 0:\n",
        "                print('Load embedding... %d' % (line_num + 1), end='\\r', flush=True)\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            word = parts[0].lower()\n",
        "            if word not in word_dict:\n",
        "                word_dict[word] = len(word_dict)\n",
        "                word_embd_weights.append(parts[1:])\n",
        "    word_embd_weights = numpy.asarray(word_embd_weights)\n",
        "    print('Dict size: %d  Shape of weights: %s' % (len(word_dict), str(word_embd_weights.shape)))\n",
        "else:\n",
        "    word_embd_weights = None\n",
        "    print('Dict size: %d' % len(word_dict))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dict size: 6848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8SiISQePCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = (len(train_sentences) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "valid_steps = (len(valid_sentences) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "\n",
        "def batch_generator(sentences, taggings, steps, training=True):\n",
        "    global word_dict\n",
        "    while True:\n",
        "        for i in range(steps):\n",
        "            batch_sentences = sentences[BATCH_SIZE * i:min(BATCH_SIZE * (i + 1), len(sentences))]\n",
        "            batch_taggings = taggings[BATCH_SIZE * i:min(BATCH_SIZE * (i + 1), len(taggings))]\n",
        "            word_input, _ = get_batch_input(\n",
        "                batch_sentences,\n",
        "                1,\n",
        "                word_dict,\n",
        "                {},\n",
        "                word_ignore_case=True,\n",
        "                char_ignore_case=False\n",
        "            )\n",
        "            if not training:\n",
        "                yield word_input, batch_taggings\n",
        "                continue\n",
        "            sentence_len = word_input.shape[1]\n",
        "            for j in range(len(batch_taggings)):\n",
        "                batch_taggings[j] = batch_taggings[j] + [0] * (sentence_len - len(batch_taggings[j]))\n",
        "                batch_taggings[j] = [[tag] for tag in batch_taggings[j]]\n",
        "            batch_taggings = numpy.asarray(batch_taggings)\n",
        "            yield word_input, batch_taggings\n",
        "        if not training:\n",
        "            break"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBfhqaQ2eTKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2477e88a-e064-40ca-f5c1-344fabee8315"
      },
      "source": [
        "model = build_model(token_num=len(word_dict),\n",
        "                    tag_num=len(TAGS))\n",
        "model.summary(line_length=80)\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_weights(MODEL_PATH, by_name=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6e332435d7f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = build_model(token_num=len(word_dict),\n\u001b[0;32m----> 2\u001b[0;31m                     tag_num=len(TAGS))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(token_num, tag_num, embedding_dim, embedding_weights, rnn_units, return_attention, lr)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mbuilt\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# sess = tf.compat.v1.Session(graph=tf.import_graph_def(), config=session_conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqe8FDOUei5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Fitting...')\n",
        "for lr in [1e-3, 1e-4, 1e-5]:\n",
        "    model.fit_generator(\n",
        "        generator=batch_generator(train_sentences, train_taggings, train_steps),\n",
        "        steps_per_epoch=train_steps,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=batch_generator(valid_sentences, valid_taggings, valid_steps),\n",
        "        validation_steps=valid_steps,\n",
        "        callbacks=[\n",
        "            keras.callbacks.EarlyStopping(monitor='val_acc', patience=5),\n",
        "        ],\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "model.save_weights(MODEL_PATH)\n",
        "\n",
        "test_sentences, test_taggings = load_data(DATA_TEST_PATH)\n",
        "test_steps = (len(valid_sentences) + BATCH_SIZE - 1) // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvICWJlFem6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Predicting...')\n",
        "def get_tags(tags):\n",
        "    filtered = []\n",
        "    for i in range(len(tags)):\n",
        "        if tags[i] == 0:\n",
        "            continue\n",
        "        if tags[i] % 2 == 1:\n",
        "            filtered.append({\n",
        "                'begin': i,\n",
        "                'end': i,\n",
        "                'type': i,\n",
        "            })\n",
        "        elif i > 0 and tags[i - 1] == tags[i] - 1:\n",
        "            filtered[-1]['end'] += 1\n",
        "    return filtered\n",
        "\n",
        "\n",
        "eps = 1e-6\n",
        "total_pred, total_true, matched_num = 0, 0, 0.0\n",
        "for inputs, batch_taggings in batch_generator(\n",
        "        train_sentences,\n",
        "        train_taggings,\n",
        "        test_steps,\n",
        "        training=False):\n",
        "    predict = model.predict_on_batch(inputs)\n",
        "    predict = numpy.argmax(predict, axis=2).tolist()\n",
        "    for i, pred in enumerate(predict):\n",
        "        pred = get_tags(pred[:len(batch_taggings[i])])\n",
        "        true = get_tags(batch_taggings[i])\n",
        "        total_pred += len(pred)\n",
        "        total_true += len(true)\n",
        "        matched_num += sum([1 for tag in pred if tag in true])\n",
        "precision = (matched_num + eps) / (total_pred + eps)\n",
        "recall = (matched_num + eps) / (total_true + eps)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "print('P: %.4f  R: %.4f  F: %.4f' % (precision, recall, f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYcUAdfpea4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Generating sample hitmap...')\n",
        "sample_text = ['pedigree', 'choice', 'cuts', 'in', 'gravy', 'with', 'beef', 'and', 'liver',\n",
        "               'canned', 'dog', 'food', '13.2', 'ounces', 'pack', 'of', '24']\n",
        "sample_input = []\n",
        "for word in sample_text:\n",
        "    if word in word_dict:\n",
        "        sample_input.append(word_dict[word])\n",
        "    else:\n",
        "        sample_input.append(word_dict['UNK'])\n",
        "sample_input = numpy.asarray([sample_input])\n",
        "\n",
        "model = build_model(token_num=len(word_dict),\n",
        "                    tag_num=len(TAGS),\n",
        "                    return_attention=True)\n",
        "model.load_weights(MODEL_PATH, by_name=True)\n",
        "attention = model.predict(sample_input)[1][0]\n",
        "ax = seaborn.heatmap(attention.tolist(),\n",
        "                     vmin=0.0,\n",
        "                     vmax=1.0,\n",
        "                     cmap='Reds',\n",
        "                     xticklabels=sample_text,\n",
        "                     yticklabels=sample_text)\n",
        "plt.savefig('sample.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}